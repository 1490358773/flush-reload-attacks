\documentclass{acm_proc_article-sp}

\usepackage{url}
\usepackage{microtype}

\begin{document}

% Without this, stuff overfills into the margin. TODO: Eventually, rewrite those
% paragraphs so that they don't.
\sloppy

\title{Related Work with Two Critiques\titlenote{Submitted October 13, 2014}}
\subtitle{Side Channel Attacks Against Non-Cryptographic Applications}

\numberofauthors{1}

\author{
\alignauthor
Taylor Hornby (Supervised by Dr. John Aycock)\\
\email{th@defuse.ca}
}

\maketitle

\begin{abstract}
This report critiques two papers related to the side-channels in
non-cryptographic applications project. For each paper, an overview of the
results, strengths, and weaknesses are given. The two papers are the
\textsc{Flush+Reload} attack \cite{yarom2013flush} and a paper presenting novel
side-channel attacks in web applications \cite{chen2010side}. This report
concludes with a list of previous work that is relevant to the project.
\end{abstract}

\section{Side-Channel Leaks in Web Applications}

The paper \cite{chen2010side} presents a new class of side-channel vulnerability
affecting web applications. It begins with an abstract model of a web
application and demonstrates how the attack works against the abstract model.
The authors then apply the attack to several popular web applications, referring
back to the abstract model to explain what is going on. Finally, they discuss
defense strategies and better development practices.

The attack exploits the fact that web applications are just like normal
applications, except that they are split into two parts -- the server and the
client -- with the Internet in between. They show that an attacker can observe
the amount of data flowing between the two parts to infer what state the web
application is in, which in turn reveals information about what its input is.
Because the attacker is only looking at the \emph{amount} of data crossing
between the client and the server, the attacks work even when the data in
transit encrypted with SSL or WPA2.

The authors show that the traffic patterns created by using an online health
website's search feature reliably reveal which medical conditions the user is
adding to their profile, even though the website is strictly SSL-encrypted. It
is also shown that the state transitions of a popular tax website reveal the
user's income range (because different tax rules apply depending on the income).
They also attack an online investment firm, as well as the Google, Yahoo, and
Bing search autocomplete.

After presenting evidence for the severity and pervasiveness of the new class of
vulnerability, the authors begin to consider defenses. They evaluate the obvious
solution of padding packets to hide their sizes and show that fixing the problem
this way would be inefficient. They argue that there is no universal fix and
that successful mitigations will have to be application-specific. Then they go
on to recommend software engineering practices to cope with the vulnerability.

\subsection{Strengths}

The paper presents a new class of side-channel attack against web applications.
This is a significant result, because it shows that common technologies like
SSL, which we previously assumed to be adequate, are not enough to protect
privacy in the applications we want to build. The paper makes this statement by
showing that several major web applications are vulnerable. The fact that such
big players in the industry are vulnerable is strong evidence that the authors
did not have to cherry-pick to find vulnerable applications. This way, the paper
successfully convinces the reader that the new vulnerabilities are both
widespread and represent real threats.

The results are presented in an understandable way, although there are some
mathematical distractions (see the next section). In particular, each concrete
attack is thoroughly explained, in enough detail that it could be reproduced
with a bit of work. The examples of attacks given are very broad, from attacking
autocomplete to infer search terms, to inferring the ratios in a pie chart by
observing small changes in the image size over time. This leaves the reader with
an impression of how varied this class of vulnerability is, so that they can
begin to intuit where such vulnerabilities might exist in their own
applications.

The authors do a good job separating their research from previous work. They
cite historical examples of side-channel attacks as well as recent research that
is similar to their own. They clearly state how their research is different and
better than what came before it. The previous work section is narrative which
makes it easy to read.

\subsection{Weaknesses}

The paper's main weakness is that it does not describe the experimental setup.
The authors don't say which web browser they did testing with, or what tools
they used to measure packet sizes. It is unclear whether they actually
implemented an attack tool and performed attacks as an experiment, or if they
just observed the application's behavior and reasoned that an attack would be
possible. Their demonstration would have been stronger if they mentioned that
they actually carried out an attack, and stronger still if they had built an
automated attack tool.

The mathematical modelling is flawed in some minor ways. To argue that web
objects usually have distinct and recognizable sizes, they look at the mean and
standard deviation of the sizes of objects they collected from various websites.
They then note that the standard deviation is large. This is a flawed analysis
because (1) They do not mention which shape the distribution is. Is it a normal
distribution? It seems unlikely, since web designers try to make all files as
small as possible. (2) The standard deviation is a measure of variance from the
mean. It is not a measure of the variance of each data point from each other,
which is what the authors intended to measure. So even though their conclusions
are correct, they don't follow from their reasoning. There are reasonable
counterexamples to their argument. Consider a website with lots of small icon
images, all of similar sizes, and lots of high-resolution images, also all of
similar sizes. The mean would be near the middle, so the standard deviation
would be large, even though the images don't have distinct and recognizable
sizes.

This problem is somewhat remedied later on in the paper when they define
``density'' as a metric that is supposed to correlate with how uniquely
identifiable different traffic flows are. They define the density, for a set of
packet sizes $P$ as $density(P) = |P| / (max\{P\} - min\{P\})$. If the density
is small, it is supposed to indicate that packet sizes are distinct and
recognizable. This measure has problems. If there are a bunch of small packets
of the same size, and one very large outlier, the difference between the max and
the min will be large, artificially reducing the density. A better metric would
be, for a multiset of packet sizes $P$, $density(P) = |P| / |supp(P)|$ where
$supp(P)$ means the set of unique elements in $P$. This is better because it is
exactly the average amount of ambiguity (number of possibilities for a given
packet size). An even better definition would account for noise in the sizes by
counting sizes near each other as ambiguous even though they are not exactly
equal. 

\section{FLUSH+RELOAD}

\textsc{Flush+Reload} \cite{yarom2013flush} is a generic L3 cache side-channel
attack. Taking advantage of executable code page sharing, it allows one process
to spy on the code another process is running. The authors apply the attack to
GnuPG 1.4.13. They show how to see which of the square, multiply, and reduce
steps of the RSA modular exponentiation algorithm are running at any given time.
By watching which steps are executed, they can extract the secret exponent,
since the steps that get executed depend directly on the bits of the secret
exponent.

\subsection{Strengths}

The \textsc{Flush+Reload} attack is more reliable than most other cache-based
side channel attacks. The authors show that the attack can be used to extract
``96.7\% of the bits of the secret key by observing a single signature or
decryption round.'' Therefore this research is significant.

The paper is well-written and describes the attack in detail, using GnuPG as
a running example. Prerequisites to understanding the attack, such as
a description of the Intel cache architecture are included so that the paper is
readable without having to refer to the Intel manuals.

Another strength is that the authors evaluate \emph{why} the error in their
attack measurements exists, concluding that it is partially caused by
speculative execution and missed time slots. Understanding where the error comes
from is important in determining how the effectiveness of the attack will
degrade on busier systems. This analysis is often missing from other
side-channel research papers.

\subsection{Weaknesses}

The paper's related work survey is lacking. The authors cite the appropriate
relevant research, but don't do a good job of comparing it to the new attack.
Some of the related work is based on similar ideas, so it's not exactly clear to
what degree the \textsc{Flush+Reload} attack is novel versus being just
a small-but-effective tweak to past attacks.

The attack implementation is not public. The authors will give you the code if
you email them. However, reproducing the attack would be much easier if the code
and reproduction instructions were available and from the time of the paper's
publication. Reproducibility is commonly missing from computer science research
\cite{moraila2013measuring}, despite its importance.

\section{Related Work List}

See the References section for a list of research that is related to this
project. Note that the list errs on the side of inclusion.

\nocite{*}

\bibliographystyle{abbrv}
\bibliography{relwork}

\end{document}

